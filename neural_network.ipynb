{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e330e82",
   "metadata": {},
   "source": [
    "# Neural Network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ce8c952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Lambda\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6550d035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "class Network:\n",
    "    \"\"\"\n",
    "    Basic Neural Network, totally unoptimized\n",
    "    Uses Stochastic Gradient Descent as the optimizer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sizes: list[int]):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "\n",
    "        # Where x is the size of the previous layer and y the size of the next layer\n",
    "        self.w = [\n",
    "            np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])\n",
    "        ]\n",
    "        self.b = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "\n",
    "    def sigmoid(self, z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Sigmoid Activation Function\"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_deriv(self, z: np.ndarray):\n",
    "        \"\"\"first derivative of sigmoid evaluated at z\"\"\"\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "\n",
    "    def forward(self, x: np.ndarray):\n",
    "        \"\"\"Forward Pass through the Network\"\"\"\n",
    "        for w, b in zip(self.w, self.b):\n",
    "            z = np.matmul(w, x) + b\n",
    "            x = self.sigmoid(z)\n",
    "        return x\n",
    "\n",
    "    def cost_deriv(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def evaluate(self, test_data: Dataset):\n",
    "        \"\"\"\n",
    "        Nr of correctly classified test-samples\n",
    "        \"\"\"\n",
    "        test_results = [(np.argmax(self.forward(x)), y) for x, y in test_data]\n",
    "        return sum(int(x == y) for x, y in test_results)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"Backpropagation\"\"\"\n",
    "        raise NotImplementedError(\"shit is not implemented yet\")\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_data: Dataset,\n",
    "        epochs: int = 20,\n",
    "        batch_size: int = 30,\n",
    "        lr: float = 0.01,\n",
    "        test_data: Optional[Dataset] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Training using Stochastic Gradient Descent.\n",
    "\n",
    "        If test_data is passed, then the network is evaluated on the test_data after each epoch\n",
    "        \"\"\"\n",
    "        if not train_data:\n",
    "            raise ValueError(\"train_data can not be none\")\n",
    "        n_train = len(train_data)\n",
    "        n_test = len(test_data) if test_data else None\n",
    "        n_batches = n_train // batch_size\n",
    "\n",
    "        test_results = []\n",
    "        for epoch in range(epochs):\n",
    "            for step in range(n_batches):\n",
    "                batch = [\n",
    "                    train_data[idx]\n",
    "                    for idx in np.random.randint(low=0, high=n_train, size=batch_size)\n",
    "                ]\n",
    "                self.minibatch_update(batch, lr)\n",
    "\n",
    "            # for x, y in batch:\n",
    "            #     y_pred = self.forward(x)\n",
    "\n",
    "            if test_data:\n",
    "                \"\"\"Compute the cost on the test set\"\"\"\n",
    "                test_result = self.evaluate(test_data)\n",
    "                test_results.append(test_result)\n",
    "                print(\n",
    "                    f\"Epoch: {epoch} / {epochs},\\t Nr of correctly classified samples: {test_result}/{n_test}\\t accuracy: {test_result / n_test:.5f}\"\n",
    "                )\n",
    "\n",
    "    def minibatch_update(\n",
    "        self, batch: List[tuple[torch.Tensor, torch.Tensor]], lr: float\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Runs one minibatch update\n",
    "\n",
    "        Key Equations of backpropagation\n",
    "\n",
    "        BP​1: δ^L =  ∇_a C ⊙ σ'(z^L)                        Get Error δ in last layer of network ∇_a C for quadratic cost 0.5(y(x) - a^L(x))² -> (a^L - y)\n",
    "        BP2: δ^l = ((w^{l+1})^T δ^{l+1}) ⊙ σ'(z^L)          Propagate Errors from last layer (BP1) through rest of network to all layers\n",
    "        BP3: ∂C/∂b^l_j = δ^l_j -> ∂C/∂b = δ                 Rate of change of cost wrp. to any bias\n",
    "        BP4: ∂C/∂w^l_jk = a^{l-1}_k δ^l_j -> a_in δ_out     Rate of change of cost wrp. to any weight\n",
    "\n",
    "        For all samples in minibatch\n",
    "            1. Get the errors and activations for all nodes\n",
    "            2. Calculate the gradients at the nodes and save them (accumulate gradients)\n",
    "\n",
    "        Calculate average gradient at each node accumulated gradients/n_samples\n",
    "        3. Update weights and biases according to update rule\n",
    "            w_k' = w_k - lr * dC/dw_k\n",
    "        \"\"\"\n",
    "        grad_w = [np.zeros(w.shape) for w in self.w]\n",
    "        grad_b = [np.zeros(b.shape) for b in self.b]\n",
    "        activations = [np.zeros_like(self.b[layer]) for layer in range(len(self.b))]\n",
    "\n",
    "        for x, y in batch:\n",
    "            # Backprop\n",
    "\n",
    "            grad_w = [np.zeros(w.shape) for w in self.w]\n",
    "            grad_b = [np.zeros(b.shape) for b in self.b]\n",
    "            activation = x\n",
    "            activations = [activation]\n",
    "            zs = []\n",
    "\n",
    "            # forward pass\n",
    "            for w, b in zip(self.w, self.b):\n",
    "                z = np.matmul(w, activation) + b\n",
    "                zs.append(z)\n",
    "                activation = self.sigmoid(z)\n",
    "                activations.append(activation)\n",
    "\n",
    "            print(len(activations))\n",
    "            print(activations[-1].shape)\n",
    "            print(y.shape)\n",
    "            delta_L = activations[-1] - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "cc90eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trns = transforms.Compose([transforms.ToTensor(), torch.nn.Flatten(), torch.squeeze])\n",
    "train_data = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=trns,\n",
    "    target_transform=Lambda(lambda y: F.one_hot(torch.tensor(y), num_classes=10)),\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=trns,\n",
    "    target_transform=Lambda(lambda y: F.one_hot(torch.tensor(y), num_classes=10)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b5e21d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "torch.Size([10, 20])\n",
      "torch.Size([10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31138/554820293.py:117: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  z = np.matmul(w, activation) + b\n",
      "/tmp/ipykernel_31138/554820293.py:22: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  return 1.0 / (1.0 + np.exp(-z))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (20) must match the size of tensor b (10) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[194]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m net = Network([\u001b[32m784\u001b[39m, \u001b[32m20\u001b[39m, \u001b[32m10\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[192]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mNetwork.train\u001b[39m\u001b[34m(self, train_data, epochs, batch_size, lr, test_data)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_batches):\n\u001b[32m     64\u001b[39m     batch = [\n\u001b[32m     65\u001b[39m         train_data[idx]\n\u001b[32m     66\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m np.random.randint(low=\u001b[32m0\u001b[39m, high=n_train, size=batch_size)\n\u001b[32m     67\u001b[39m     ]\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mminibatch_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# for x, y in batch:\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m#     y_pred = self.forward(x)\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m test_data:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[192]\u001b[39m\u001b[32m, line 125\u001b[39m, in \u001b[36mNetwork.minibatch_update\u001b[39m\u001b[34m(self, batch, lr)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28mprint\u001b[39m(activations[-\u001b[32m1\u001b[39m].shape)\n\u001b[32m    124\u001b[39m \u001b[38;5;28mprint\u001b[39m(y.shape)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m delta_L = \u001b[43mactivations\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (20) must match the size of tensor b (10) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "net = Network([784, 20, 10])\n",
    "net.train(train_data, epochs=20, batch_size=30, lr=0.01, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebc17cf",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39204bfa",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc142fa6",
   "metadata": {},
   "source": [
    "Network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-network-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
